{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "week9q1submit.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YKbTLI8Pkde",
        "outputId": "cb55aff6-ca56-4ac9-e5e2-4b59269ee677"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-u44h45ie\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-u44h45ie\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp37-none-any.whl size=4307 sha256=dcaf3cff6515ac71cf45e0372416884bb71f5c62e6004ce2d7b6dc90f9a9af25\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p5p9080_/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_sFcjfyPvx-",
        "outputId": "fafb018a-6bcb-4d4f-8e89-475b0bc8f0e5"
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include<stdio.h>\n",
        "#include<stdlib.h>\n",
        "#define Mask_Width 5\n",
        "#define TILE_SIZE 4\n",
        " \n",
        "__constant__ int d_mask[Mask_Width];\n",
        " \n",
        "__global__ void Convolution_constant(int *src, int *res, int m_width, int src_length){\n",
        "    //taking the threadid\n",
        "    int id =  threadIdx.x;\n",
        "    if(id < src_length){\n",
        "        //declaring the start point\n",
        "        int start = id - (m_width / 2);\n",
        "        int pval = 0;\n",
        "    \n",
        "       //Looping throught the array and multiplying with the mask array\n",
        "        for(int i = 0; i < m_width; i++){\n",
        "            if((start + i) >= 0 && (start + i) < src_length){\n",
        "               // printf(\"Elements being multiplied are: src = %d mask = %d\\n\", src[start + i], d_mask[i]);\n",
        "                pval += (src[start + i] * d_mask[i]);\n",
        "               // printf(\"PVal = %d\\n\", pval);\n",
        "            }\n",
        "        }\n",
        " \n",
        "        //storing the answer in the resultant array\n",
        " res[id] = pval;        \n",
        "      }\n",
        "}\n",
        " \n",
        "__global__ void Convolution_shared(int *eles, int *ans, int eles_length){\n",
        "    int id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    \n",
        "      //Initializing the shared memory for elements\n",
        "    __shared__ int shared_Src[TILE_SIZE + Mask_Width - 1];\n",
        " \n",
        "    int s = Mask_Width / 2;\n",
        "    \n",
        "    if(id < eles_length){\n",
        "            //Populating the shared memory\n",
        " \n",
        "    //calculating the left halo indices\n",
        "    int halo_left = (blockIdx.x - 1) * blockDim.x + threadIdx.x;\n",
        "    if(threadIdx.x >= (blockDim.x - s)){\n",
        "        shared_Src[threadIdx.x - (blockDim.x - s)] = (halo_left < 0) ? 0: eles[halo_left];\n",
        "    }\n",
        "    shared_Src[s + threadIdx.x] = eles[blockIdx.x * blockDim.x + threadIdx.x];\n",
        " \n",
        "    //Calculating the right halo indices\n",
        "    int halo_right = (blockIdx.x + 1) * blockDim.x + threadIdx.x;\n",
        "    if(threadIdx.x < s){\n",
        "        shared_Src[s + blockDim.x + threadIdx.x] = (halo_right >= eles_length) ? 0 : eles[halo_right]; \n",
        "    }\n",
        "    __syncthreads();\n",
        " \n",
        "    //Calculating the resultant array\n",
        "    int pval = 0;\n",
        "    for(int i = 0; i < Mask_Width; i++){\n",
        "        pval += (shared_Src[threadIdx.x + i] * d_mask[i]);\n",
        "    }\n",
        " \n",
        "    //Storing the result\n",
        "    ans[id] = pval;\n",
        "        \n",
        "    }\n",
        "}\n",
        " \n",
        "int main(){\n",
        "    //Initializing the input array and the mask array\n",
        "    int n = 8;\n",
        "    int input [] = {3, 4, 15, 4, 67, 89, 12, 5};\n",
        "    int mask[] = {7, 8, 9, 10, 11};\n",
        "    int size_input = sizeof(int) * n;\n",
        "    int size_mask = sizeof(int) * Mask_Width;\n",
        "    int h_output[n], h_output_s[n];\n",
        "\n",
        "    //to mark the time taken by the kernel to execute \n",
        "    float et;\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start); cudaEventCreate(&stop);\n",
        " \n",
        "    //Allocating space in the device\n",
        "    int *d_input, *d_output, *d_output_s;\n",
        "    cudaMalloc((void **)&d_input, size_input);\n",
        "    cudaMalloc((void **)&d_output, size_input);\n",
        "    cudaMalloc((void **)&d_output_s, size_input);\n",
        " \n",
        "    //Copying the mask directly to constant memory\n",
        "    cudaMemcpyToSymbol(d_mask, mask, size_mask);\n",
        " \n",
        "    //Copying the input to the device memory\n",
        "    cudaMemcpy(d_input, input, size_input, cudaMemcpyHostToDevice);\n",
        " \n",
        "    //Calling the kernel function\n",
        "    int threads = 16;\n",
        "    int blocks = (threads + n - 1) / threads;\n",
        "    \n",
        "    //starting the recorder to calc the execution time \n",
        "    cudaEventRecord(start);\n",
        "    \n",
        "    Convolution_constant<<<blocks, threads>>>(d_input, d_output, 5, n);\n",
        "    \n",
        "    cudaEventRecord(stop);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    //Calculating the elapsed time for constant memory kernel\n",
        "    cudaEventElapsedTime(&et, start, stop);\n",
        "    printf(\"\\n Time taken by constant memory kernel to execute is: %f\\n\", et);\n",
        " \n",
        "    //calling the shared memory kernel\n",
        "    cudaEvent_t start_s, stop_s;\n",
        "    cudaEventCreate(&start_s); cudaEventCreate(&stop_s);\n",
        "    threads = 4;\n",
        "    blocks = (threads + n - 1) / threads;\n",
        "\n",
        "    //similarly as before starting the timer to calcualte the execution time \n",
        "    // this is for the shared memory kernel\n",
        "    cudaEventRecord(start_s);\n",
        "    Convolution_shared<<<blocks, threads>>>(d_input, d_output_s, n);\n",
        "    cudaEventRecord(stop_s);\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaEventElapsedTime(&et, start_s, stop_s);\n",
        " \n",
        "    //printing the elapsed time of shared kernel\n",
        "    printf(\"\\nThe time elapsed of the shared memory kernel is: %f\\n\", et);\n",
        "    \n",
        "    //Copying the constant memory result to host\n",
        "    cudaMemcpy(h_output, d_output, size_input, cudaMemcpyDeviceToHost);\n",
        " \n",
        "    //Copying the shared memory result to host\n",
        "    cudaMemcpy(h_output_s, d_output_s, size_input, cudaMemcpyDeviceToHost);\n",
        " \n",
        "    //printing the result\n",
        "    printf(\"\\n Input array: > \\n\");\n",
        "    for(int i = 0; i < n; i++){\n",
        "        printf(\"%d\\t\", input[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "    printf(\"\\n Mask array: > \\n\");\n",
        "    for(int i = 0; i < Mask_Width; i++){\n",
        "        printf(\"%d\\t\", mask[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "    printf(\"\\nPart 1>  Constant memory resultant array:\\n\");\n",
        "    for(int i = 0; i < n; i++){\n",
        "        printf(\"%d\\t\", h_output[i]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "    printf(\"\\nPart 2> Constant shared resultant array:\\n\");\n",
        "    for(int i = 0; i < n; i++){\n",
        "        printf(\"%d\\t\", h_output_s[i]);\n",
        "    }\n",
        " \n",
        "    //Freeing the allocated device memory\n",
        "    cudaFree(d_output);\n",
        "    cudaFree(d_output_s);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_mask);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Time taken by constant memory kernel to execute is: 0.018848\n",
            "\n",
            "The time elapsed of the shared memory kernel is: 0.008224\n",
            "\n",
            " Input array: > \n",
            "3\t4\t15\t4\t67\t89\t12\t5\t\n",
            "\n",
            " Mask array: > \n",
            "7\t8\t9\t10\t11\t\n",
            "\n",
            "Part 1>  Constant memory resultant array:\n",
            "232\t254\t965\t1833\t1762\t1540\t1339\t764\t\n",
            "\n",
            "Part 2> Constant shared resultant array:\n",
            "232\t254\t965\t1833\t1762\t1540\t1339\t764\t\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}